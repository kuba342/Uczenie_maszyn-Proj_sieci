{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c871fa1-528b-4309-a1cc-9ad0de00c8fa",
   "metadata": {},
   "source": [
    "# Próba wykorzystania istniejącego modelu RegGNN\n",
    "https://github.com/basiralab/RegGNN/blob/main/proposed_method/RegGNN.py\n",
    "\n",
    "Nasze zadanie opiera się na przygotowaniu modelu rozwiązującego zadanie regresji grafowej, czyli na podstawie grafów, ich cech globalnych i etykiet chcemy prognozować wartość metryki średniej ilości wykorzystanych transceiverów podczas symulacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9deef8a4-17e1-4edd-ba6f-3f19d15a96a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Wczytaj zbiór uczący\n",
    "train_dataset = torch.load('train_dataset.pt')\n",
    "# wczytaj zbiór testowy\n",
    "test_dataset = torch.load('test_dataset.pt')\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88cbb7-71a2-4e0f-b89f-2758953ecd6f",
   "metadata": {},
   "source": [
    "# Przykładowy model regresyjny Grafowej sieci neuronowej GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c9d06d-d65d-4fd8-be44-9eff45edd0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "\n",
    "class GINRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(GINRegression, self).__init__()\n",
    "\n",
    "        # GINConv layers\n",
    "        self.conv1 = GINConv(nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ), train_eps=True)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GINConv(nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ), train_eps=True))\n",
    "\n",
    "        # Fully connected layers for regression\n",
    "        self.fc1 = nn.Linear(hidden_dim + 2, hidden_dim)  # Dodane 2 na cechy globalne\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        density, avg_clustering = data.global_feature[:, 0], data.global_feature[:, 1]  # Wyciąganie globalnych cech\n",
    "\n",
    "        # GINConv layers\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "\n",
    "        # Global pooling and fully connected layers with global features\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = torch.cat([x, density.view(-1, 1), avg_clustering.view(-1, 1)], dim=1)  # Dodane globalne cechy\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def loss(self, pred, score):\n",
    "        return F.mse_loss(pred, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea28cb8-2d89-4112-880d-372fd8f142e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "class GraphRegressionModel(nn.Module):\n",
    "    def __init__(self, num_node_features=1, hidden_dim=64, output_dim=1, dropout=0.5):\n",
    "        super(GraphRegressionModel, self).__init__()\n",
    "\n",
    "        # Graph Convolutional Layer\n",
    "        self.conv1 = GraphConv(num_node_features, hidden_dim)\n",
    "\n",
    "        # Fully Connected Layers with Dropout\n",
    "        self.fc1 = nn.Linear(hidden_dim + 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, global_feature = data.x, data.edge_index, data.edge_weight, data.global_feature\n",
    "\n",
    "        # Apply Graph Convolution\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "\n",
    "        # Global features concatenation\n",
    "        global_feature = global_feature.expand(x.size(0), -1)  # Dostosuj global_feature do rozmiaru x\n",
    "        x = torch.cat([x, global_feature], dim=1)\n",
    "\n",
    "        # Fully Connected Layers with Dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def loss(self, pred, score):\n",
    "        return F.mse_loss(pred, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "900c11d2-46cc-4d45-88fb-ba53eced7db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jderd\\AppData\\Local\\Temp\\ipykernel_21620\\794946686.py:36: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(pred, score)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:39<00:00, 69.90s/trial, best loss: 53519.48545837402]\n",
      "Najlepsze hiperparametry: {'dropout': 0.06769502094656675, 'hidden_dim': 192.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "# Zdefiniuj funkcję oceny (score function)\n",
    "def objective(params):\n",
    "    hidden_dim = int(params['hidden_dim'])\n",
    "    dropout = params['dropout']\n",
    "\n",
    "    model = GraphRegressionModel(hidden_dim=hidden_dim, dropout=dropout)\n",
    "    \n",
    "    # Definiuj optymalizator, liczbę epok, itp.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 200\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Trening i ewaluacja modelu\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, data.y.view(-1, 1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Ewaluacja na zbiorze testowym\n",
    "    test_loss = 0.0  \n",
    "    for data in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += model.loss(output, data.y.view(-1, 1).float()).item()\n",
    "    \n",
    "    # Zwróć funkcję oceny (score)\n",
    "    return test_loss\n",
    "\n",
    "# Przestrzeń poszukiwań dla hyperopt\n",
    "space = {\n",
    "    'hidden_dim': hp.quniform('hidden_dim', 32, 256, 32),  # Przeszukuj wartości co 32\n",
    "    'dropout': hp.uniform('dropout', 0.05, 0.8),\n",
    "}\n",
    "\n",
    "# Minimalizacja funkcji oceny za pomocą algorytmu TPE\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10)  # Dla przykładu ustawiono max_evals na 10\n",
    "print(\"Najlepsze hiperparametry:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e498ea76-9b7d-4e54-9f01-f974f6269914",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jderd\\AppData\\Local\\Temp\\ipykernel_21620\\794946686.py:36: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(pred, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n",
      "Epoch: 108\n",
      "Epoch: 109\n",
      "Epoch: 110\n",
      "Epoch: 111\n",
      "Epoch: 112\n",
      "Epoch: 113\n",
      "Epoch: 114\n",
      "Epoch: 115\n",
      "Epoch: 116\n",
      "Epoch: 117\n",
      "Epoch: 118\n",
      "Epoch: 119\n",
      "Epoch: 120\n",
      "Epoch: 121\n",
      "Epoch: 122\n",
      "Epoch: 123\n",
      "Epoch: 124\n",
      "Epoch: 125\n",
      "Epoch: 126\n",
      "Epoch: 127\n",
      "Epoch: 128\n",
      "Epoch: 129\n",
      "Epoch: 130\n",
      "Epoch: 131\n",
      "Epoch: 132\n",
      "Epoch: 133\n",
      "Epoch: 134\n",
      "Epoch: 135\n",
      "Epoch: 136\n",
      "Epoch: 137\n",
      "Epoch: 138\n",
      "Epoch: 139\n",
      "Epoch: 140\n",
      "Epoch: 141\n",
      "Epoch: 142\n",
      "Epoch: 143\n",
      "Epoch: 144\n",
      "Epoch: 145\n",
      "Epoch: 146\n",
      "Epoch: 147\n",
      "Epoch: 148\n",
      "Epoch: 149\n",
      "Epoch: 150\n",
      "Epoch: 151\n",
      "Epoch: 152\n",
      "Epoch: 153\n",
      "Epoch: 154\n",
      "Epoch: 155\n",
      "Epoch: 156\n",
      "Epoch: 157\n",
      "Epoch: 158\n",
      "Epoch: 159\n",
      "Epoch: 160\n",
      "Epoch: 161\n",
      "Epoch: 162\n",
      "Epoch: 163\n",
      "Epoch: 164\n",
      "Epoch: 165\n",
      "Epoch: 166\n",
      "Epoch: 167\n",
      "Epoch: 168\n",
      "Epoch: 169\n",
      "Epoch: 170\n",
      "Epoch: 171\n",
      "Epoch: 172\n",
      "Epoch: 173\n",
      "Epoch: 174\n",
      "Epoch: 175\n",
      "Epoch: 176\n",
      "Epoch: 177\n",
      "Epoch: 178\n",
      "Epoch: 179\n",
      "Epoch: 180\n",
      "Epoch: 181\n",
      "Epoch: 182\n",
      "Epoch: 183\n",
      "Epoch: 184\n",
      "Epoch: 185\n",
      "Epoch: 186\n",
      "Epoch: 187\n",
      "Epoch: 188\n",
      "Epoch: 189\n",
      "Epoch: 190\n",
      "Epoch: 191\n",
      "Epoch: 192\n",
      "Epoch: 193\n",
      "Epoch: 194\n",
      "Epoch: 195\n",
      "Epoch: 196\n",
      "Epoch: 197\n",
      "Epoch: 198\n",
      "Epoch: 199\n",
      "Przewidywana wartość: 834.1890869140625\n",
      "Rzeczywista wartość: 874.5499877929688\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ustawienie optymalnych hiperparametrów\n",
    "optimal_hidden_dim = int(best['hidden_dim'])\n",
    "optimal_dropout = best['dropout']\n",
    "\n",
    "# Inicjalizacja modelu z optymalnymi hiperparametrami\n",
    "optimal_model = GraphRegressionModel(hidden_dim=optimal_hidden_dim, dropout=optimal_dropout)\n",
    "\n",
    "# Definiuj DataLoader dla zbiorów uczącego i testowego\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Definiuj optymalizator, liczbę epok, itp.\n",
    "optimizer = torch.optim.Adam(optimal_model.parameters(), lr=0.001)\n",
    "num_epochs = 200\n",
    "\n",
    "# Listy do śledzenia train loss i test loss\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Trening modelu\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = optimal_model(data)\n",
    "        loss = optimal_model.loss(output, data.y.view(-1, 1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "sample_data = test_dataset[0]\n",
    "\n",
    "# Przeprowadzenie predykcji na przykładowym obiekcie\n",
    "optimal_model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = optimal_model(sample_data)[0][0].item()\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Przewidywana wartość:\", prediction)\n",
    "print(\"Rzeczywista wartość:\", sample_data.y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40fc4a8c-0a49-4e5e-8fed-c54bb4991be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana wartość: 830.5301513671875\n",
      "Rzeczywista wartość: 860.9299926757812\n"
     ]
    }
   ],
   "source": [
    "sample_data = test_dataset[30]\n",
    "\n",
    "# Przeprowadzenie predykcji na przykładowym obiekcie\n",
    "optimal_model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = optimal_model(sample_data)[0][0].item()\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Przewidywana wartość:\", prediction)\n",
    "print(\"Rzeczywista wartość:\", sample_data.y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4e0e9f9-d9b9-487b-a673-1a3ea0cd313c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana wartość: 287.41595458984375\n",
      "Rzeczywista wartość: 286.3299865722656\n"
     ]
    }
   ],
   "source": [
    "sample_data = test_dataset[9]\n",
    "\n",
    "# Przeprowadzenie predykcji na przykładowym obiekcie\n",
    "optimal_model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = optimal_model(sample_data)[0][0].item()\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Przewidywana wartość:\", prediction)\n",
    "print(\"Rzeczywista wartość:\", sample_data.y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5e5f93d-8db8-4a92-8058-f4ce2fa5f729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykład 1:\n",
      "Przewidywana wartość: 834.1890869140625\n",
      "Rzeczywista wartość: 874.5499877929688\n",
      "Procentowa jakość regresji: 95.38%\n",
      "\n",
      "Przykład 2:\n",
      "Przewidywana wartość: 576.5999145507812\n",
      "Rzeczywista wartość: 569.8499755859375\n",
      "Procentowa jakość regresji: 98.82%\n",
      "\n",
      "Przykład 3:\n",
      "Przewidywana wartość: 461.2392578125\n",
      "Rzeczywista wartość: 450.54998779296875\n",
      "Procentowa jakość regresji: 97.63%\n",
      "\n",
      "Przykład 4:\n",
      "Przewidywana wartość: 881.8521118164062\n",
      "Rzeczywista wartość: 839.4600219726562\n",
      "Procentowa jakość regresji: 94.95%\n",
      "\n",
      "Przykład 5:\n",
      "Przewidywana wartość: 702.5387573242188\n",
      "Rzeczywista wartość: 705.010009765625\n",
      "Procentowa jakość regresji: 99.65%\n",
      "\n",
      "Przykład 6:\n",
      "Przewidywana wartość: 687.1488037109375\n",
      "Rzeczywista wartość: 697.9400024414062\n",
      "Procentowa jakość regresji: 98.45%\n",
      "\n",
      "Przykład 7:\n",
      "Przewidywana wartość: 870.410400390625\n",
      "Rzeczywista wartość: 854.9099731445312\n",
      "Procentowa jakość regresji: 98.19%\n",
      "\n",
      "Przykład 8:\n",
      "Przewidywana wartość: 800.1185913085938\n",
      "Rzeczywista wartość: 813.02001953125\n",
      "Procentowa jakość regresji: 98.41%\n",
      "\n",
      "Przykład 9:\n",
      "Przewidywana wartość: 313.1221618652344\n",
      "Rzeczywista wartość: 286.0299987792969\n",
      "Procentowa jakość regresji: 90.53%\n",
      "\n",
      "Przykład 10:\n",
      "Przewidywana wartość: 287.41595458984375\n",
      "Rzeczywista wartość: 286.3299865722656\n",
      "Procentowa jakość regresji: 99.62%\n",
      "\n",
      "Przykład 11:\n",
      "Przewidywana wartość: 636.6835327148438\n",
      "Rzeczywista wartość: 673.02001953125\n",
      "Procentowa jakość regresji: 94.60%\n",
      "\n",
      "Przykład 12:\n",
      "Przewidywana wartość: 188.58734130859375\n",
      "Rzeczywista wartość: 198.0\n",
      "Procentowa jakość regresji: 95.25%\n",
      "\n",
      "Przykład 13:\n",
      "Przewidywana wartość: 868.7130737304688\n",
      "Rzeczywista wartość: 863.3800048828125\n",
      "Procentowa jakość regresji: 99.38%\n",
      "\n",
      "Przykład 14:\n",
      "Przewidywana wartość: 262.591552734375\n",
      "Rzeczywista wartość: 240.9499969482422\n",
      "Procentowa jakość regresji: 91.02%\n",
      "\n",
      "Przykład 15:\n",
      "Przewidywana wartość: 245.71690368652344\n",
      "Rzeczywista wartość: 238.02999877929688\n",
      "Procentowa jakość regresji: 96.77%\n",
      "\n",
      "Przykład 16:\n",
      "Przewidywana wartość: 798.134765625\n",
      "Rzeczywista wartość: 855.3099975585938\n",
      "Procentowa jakość regresji: 93.32%\n",
      "\n",
      "Przykład 17:\n",
      "Przewidywana wartość: 389.4044494628906\n",
      "Rzeczywista wartość: 367.8999938964844\n",
      "Procentowa jakość regresji: 94.15%\n",
      "\n",
      "Przykład 18:\n",
      "Przewidywana wartość: 602.3310546875\n",
      "Rzeczywista wartość: 620.719970703125\n",
      "Procentowa jakość regresji: 97.04%\n",
      "\n",
      "Przykład 19:\n",
      "Przewidywana wartość: 843.0896606445312\n",
      "Rzeczywista wartość: 828.6599731445312\n",
      "Procentowa jakość regresji: 98.26%\n",
      "\n",
      "Przykład 20:\n",
      "Przewidywana wartość: 488.8903503417969\n",
      "Rzeczywista wartość: 485.5199890136719\n",
      "Procentowa jakość regresji: 99.31%\n",
      "\n",
      "Przykład 21:\n",
      "Przewidywana wartość: 294.8210144042969\n",
      "Rzeczywista wartość: 291.5199890136719\n",
      "Procentowa jakość regresji: 98.87%\n",
      "\n",
      "Przykład 22:\n",
      "Przewidywana wartość: 346.6177978515625\n",
      "Rzeczywista wartość: 325.3299865722656\n",
      "Procentowa jakość regresji: 93.46%\n",
      "\n",
      "Przykład 23:\n",
      "Przewidywana wartość: 409.6965026855469\n",
      "Rzeczywista wartość: 382.94000244140625\n",
      "Procentowa jakość regresji: 93.01%\n",
      "\n",
      "Przykład 24:\n",
      "Przewidywana wartość: 438.1330871582031\n",
      "Rzeczywista wartość: 416.5400085449219\n",
      "Procentowa jakość regresji: 94.82%\n",
      "\n",
      "Przykład 25:\n",
      "Przewidywana wartość: 906.2385864257812\n",
      "Rzeczywista wartość: 895.22998046875\n",
      "Procentowa jakość regresji: 98.77%\n",
      "\n",
      "Przykład 26:\n",
      "Przewidywana wartość: 958.824462890625\n",
      "Rzeczywista wartość: 903.760009765625\n",
      "Procentowa jakość regresji: 93.91%\n",
      "\n",
      "Przykład 27:\n",
      "Przewidywana wartość: 827.7879638671875\n",
      "Rzeczywista wartość: 816.4500122070312\n",
      "Procentowa jakość regresji: 98.61%\n",
      "\n",
      "Przykład 28:\n",
      "Przewidywana wartość: 862.19580078125\n",
      "Rzeczywista wartość: 858.4500122070312\n",
      "Procentowa jakość regresji: 99.56%\n",
      "\n",
      "Przykład 29:\n",
      "Przewidywana wartość: 787.8715209960938\n",
      "Rzeczywista wartość: 806.6900024414062\n",
      "Procentowa jakość regresji: 97.67%\n",
      "\n",
      "Przykład 30:\n",
      "Przewidywana wartość: 210.07562255859375\n",
      "Rzeczywista wartość: 200.39999389648438\n",
      "Procentowa jakość regresji: 95.17%\n",
      "\n",
      "Przykład 31:\n",
      "Przewidywana wartość: 830.5301513671875\n",
      "Rzeczywista wartość: 860.9299926757812\n",
      "Procentowa jakość regresji: 96.47%\n",
      "\n",
      "Przykład 32:\n",
      "Przewidywana wartość: 891.6096801757812\n",
      "Rzeczywista wartość: 898.5999755859375\n",
      "Procentowa jakość regresji: 99.22%\n",
      "\n",
      "Przykład 33:\n",
      "Przewidywana wartość: 800.7440795898438\n",
      "Rzeczywista wartość: 786.8599853515625\n",
      "Procentowa jakość regresji: 98.24%\n",
      "\n",
      "Przykład 34:\n",
      "Przewidywana wartość: 477.6459045410156\n",
      "Rzeczywista wartość: 463.6000061035156\n",
      "Procentowa jakość regresji: 96.97%\n",
      "\n",
      "Przykład 35:\n",
      "Przewidywana wartość: 664.0677490234375\n",
      "Rzeczywista wartość: 640.3599853515625\n",
      "Procentowa jakość regresji: 96.30%\n",
      "\n",
      "Przykład 36:\n",
      "Przewidywana wartość: 936.4877319335938\n",
      "Rzeczywista wartość: 979.5999755859375\n",
      "Procentowa jakość regresji: 95.60%\n",
      "\n",
      "Przykład 37:\n",
      "Przewidywana wartość: 195.85824584960938\n",
      "Rzeczywista wartość: 197.47000122070312\n",
      "Procentowa jakość regresji: 99.18%\n",
      "\n",
      "Przykład 38:\n",
      "Przewidywana wartość: 703.8026733398438\n",
      "Rzeczywista wartość: 678.4500122070312\n",
      "Procentowa jakość regresji: 96.26%\n",
      "\n",
      "Przykład 39:\n",
      "Przewidywana wartość: 400.3258056640625\n",
      "Rzeczywista wartość: 368.6300048828125\n",
      "Procentowa jakość regresji: 91.40%\n",
      "\n",
      "Przykład 40:\n",
      "Przewidywana wartość: 243.45547485351562\n",
      "Rzeczywista wartość: 246.77999877929688\n",
      "Procentowa jakość regresji: 98.65%\n",
      "\n",
      "Przykład 41:\n",
      "Przewidywana wartość: 599.9741821289062\n",
      "Rzeczywista wartość: 588.5499877929688\n",
      "Procentowa jakość regresji: 98.06%\n",
      "\n",
      "Przykład 42:\n",
      "Przewidywana wartość: 873.4169921875\n",
      "Rzeczywista wartość: 891.1699829101562\n",
      "Procentowa jakość regresji: 98.01%\n",
      "\n",
      "Przykład 43:\n",
      "Przewidywana wartość: 961.0892944335938\n",
      "Rzeczywista wartość: 916.719970703125\n",
      "Procentowa jakość regresji: 95.16%\n",
      "\n",
      "Przykład 44:\n",
      "Przewidywana wartość: 574.576904296875\n",
      "Rzeczywista wartość: 551.02001953125\n",
      "Procentowa jakość regresji: 95.72%\n",
      "\n",
      "Przykład 45:\n",
      "Przewidywana wartość: 511.0352783203125\n",
      "Rzeczywista wartość: 501.6300048828125\n",
      "Procentowa jakość regresji: 98.13%\n",
      "\n",
      "Przykład 46:\n",
      "Przewidywana wartość: 754.2468872070312\n",
      "Rzeczywista wartość: 778.0399780273438\n",
      "Procentowa jakość regresji: 96.94%\n",
      "\n",
      "Średnia procentowa jakość regresji dla wszystkich przykładów: 96.63%\n"
     ]
    }
   ],
   "source": [
    "total_accuracy = 0.0\n",
    "\n",
    "# Iteracja przez cały zbiór testowy\n",
    "for i, sample_data in enumerate(test_dataset):\n",
    "    # Przeprowadzenie predykcji na przykładowym obiekcie\n",
    "    optimal_model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = optimal_model(sample_data)[0][0].item()\n",
    "    \n",
    "    # Wyświetlenie wyników\n",
    "    print(f\"Przykład {i + 1}:\")\n",
    "    print(\"Przewidywana wartość:\", prediction)\n",
    "    print(\"Rzeczywista wartość:\", sample_data.y.item())\n",
    "    \n",
    "    # Obliczenie procentowej jakości regresji\n",
    "    actual_value = sample_data.y.item()\n",
    "    accuracy = 100 * (1 - abs(prediction - actual_value) / actual_value)\n",
    "    total_accuracy += accuracy\n",
    "    \n",
    "    print(f\"Procentowa jakość regresji: {accuracy:.2f}%\\n\")\n",
    "\n",
    "# Obliczenie średniej jakości procentowej\n",
    "average_accuracy = total_accuracy / len(test_dataset)\n",
    "print(f\"Średnia procentowa jakość regresji dla wszystkich przykładów: {average_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acce16a7-2830-4d54-ba13-bd2a170c4478",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphRegressionModel(\n",
      "  (conv1): GraphConv(1, 64)\n",
      "  (fc1): Linear(in_features=66, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Definiuj DataLoader dla zbiorów uczącego i testowego\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Przykładowe użycie modelu\n",
    "input_dim = 1  # Brak cech wierzchołków, jedna cecha domyślna\n",
    "hidden_dim = 192  # liczba neuronów w warstwie ukrytej\n",
    "output_dim = 1   # wymiar wyjścia (regresja)\n",
    "num_layers = 3   # liczba warstw GINConv\n",
    "dropout = 0.05    # prawdopodobieństwo dropoutu\n",
    "\n",
    "# model = GINRegression(input_dim, hidden_dim, output_dim, num_layers, dropout).float().to(device)\n",
    "model = GraphRegressionModel(dropout=dropout)\n",
    "print(model)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71edd3f-fb3a-41a1-b956-cbad4189d8fb",
   "metadata": {},
   "source": [
    "# Trening modelu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629b8c36-9def-401e-aad1-ca7a2c929b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trenuj model\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    print(len(train_loader))\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #output = model(data.x, data.edge_index)\n",
    "        loss = model.loss(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17be206-1cc6-4ba9-8bf3-169eb96ead6f",
   "metadata": {},
   "source": [
    "# Ewaluacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5e6797-8721-4681-88f9-01c2dcd03024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        output = model(data)\n",
    "        #output = model(data.x, data.edge_index)\n",
    "        total_loss += model.loss(output, data.y).item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db880bd-f2a6-45ce-868d-f18fab66f4f3",
   "metadata": {},
   "source": [
    "# Wywołanie treningu i ewaluacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8ae2c4-78c7-45ca-9e70-ea3c25daf972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jderd\\AppData\\Local\\Temp\\ipykernel_21620\\794946686.py:36: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(pred, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 51072.6552, Test Loss: 60262.2976\n",
      "184\n",
      "Epoch: 2, Train Loss: 52569.5933, Test Loss: 60528.7949\n",
      "184\n",
      "Epoch: 3, Train Loss: 49015.4461, Test Loss: 59581.5037\n",
      "184\n",
      "Epoch: 4, Train Loss: 51250.5943, Test Loss: 58801.5894\n",
      "184\n",
      "Epoch: 5, Train Loss: 49117.6971, Test Loss: 56942.9928\n",
      "184\n",
      "Epoch: 6, Train Loss: 51634.6756, Test Loss: 58249.3101\n",
      "184\n",
      "Epoch: 7, Train Loss: 45260.2548, Test Loss: 55248.4855\n",
      "184\n",
      "Epoch: 8, Train Loss: 45090.2410, Test Loss: 55438.6659\n",
      "184\n",
      "Epoch: 9, Train Loss: 45806.8905, Test Loss: 52549.7588\n",
      "184\n",
      "Epoch: 10, Train Loss: 48682.4704, Test Loss: 54181.5169\n",
      "184\n",
      "Epoch: 11, Train Loss: 39898.5331, Test Loss: 47531.2800\n",
      "184\n",
      "Epoch: 12, Train Loss: 39413.5759, Test Loss: 46080.1255\n",
      "184\n",
      "Epoch: 13, Train Loss: 36264.2882, Test Loss: 44161.8646\n",
      "184\n",
      "Epoch: 14, Train Loss: 35619.3404, Test Loss: 41650.8509\n",
      "184\n",
      "Epoch: 15, Train Loss: 32428.7488, Test Loss: 39838.1597\n",
      "184\n",
      "Epoch: 16, Train Loss: 30563.0840, Test Loss: 36055.5909\n",
      "184\n",
      "Epoch: 17, Train Loss: 28420.4653, Test Loss: 33300.5054\n",
      "184\n",
      "Epoch: 18, Train Loss: 25969.3760, Test Loss: 30273.5475\n",
      "184\n",
      "Epoch: 19, Train Loss: 21960.3459, Test Loss: 26362.8689\n",
      "184\n",
      "Epoch: 20, Train Loss: 22905.3331, Test Loss: 25627.2419\n",
      "184\n",
      "Epoch: 21, Train Loss: 19452.3241, Test Loss: 21806.3572\n",
      "184\n",
      "Epoch: 22, Train Loss: 13352.0659, Test Loss: 16512.8094\n",
      "184\n",
      "Epoch: 23, Train Loss: 10964.3079, Test Loss: 13793.5579\n",
      "184\n",
      "Epoch: 24, Train Loss: 8758.0607, Test Loss: 11119.5279\n",
      "184\n",
      "Epoch: 25, Train Loss: 6464.6609, Test Loss: 8233.6520\n",
      "184\n",
      "Epoch: 26, Train Loss: 4672.9315, Test Loss: 5905.2466\n",
      "184\n",
      "Epoch: 27, Train Loss: 3336.8480, Test Loss: 4276.8474\n",
      "184\n",
      "Epoch: 28, Train Loss: 2665.3096, Test Loss: 3488.6435\n",
      "184\n",
      "Epoch: 29, Train Loss: 1705.2939, Test Loss: 2207.9381\n",
      "184\n",
      "Epoch: 30, Train Loss: 1452.6126, Test Loss: 1699.2543\n",
      "184\n",
      "Epoch: 31, Train Loss: 1025.2343, Test Loss: 1217.4701\n",
      "184\n",
      "Epoch: 32, Train Loss: 896.5753, Test Loss: 1008.5054\n",
      "184\n",
      "Epoch: 33, Train Loss: 665.0010, Test Loss: 761.8805\n",
      "184\n",
      "Epoch: 34, Train Loss: 735.0481, Test Loss: 789.3454\n",
      "184\n",
      "Epoch: 35, Train Loss: 559.5315, Test Loss: 656.4596\n",
      "184\n",
      "Epoch: 36, Train Loss: 941.7566, Test Loss: 931.3823\n",
      "184\n",
      "Epoch: 37, Train Loss: 975.8273, Test Loss: 951.6485\n",
      "184\n",
      "Epoch: 38, Train Loss: 537.0067, Test Loss: 578.4714\n",
      "184\n",
      "Epoch: 39, Train Loss: 530.9703, Test Loss: 574.2887\n",
      "184\n",
      "Epoch: 40, Train Loss: 764.7629, Test Loss: 766.9475\n",
      "184\n",
      "Epoch: 41, Train Loss: 536.2888, Test Loss: 577.9554\n",
      "184\n",
      "Epoch: 42, Train Loss: 596.2307, Test Loss: 614.4677\n",
      "184\n",
      "Epoch: 43, Train Loss: 708.6974, Test Loss: 714.2984\n",
      "184\n",
      "Epoch: 44, Train Loss: 550.8375, Test Loss: 584.8288\n",
      "184\n",
      "Epoch: 45, Train Loss: 585.5912, Test Loss: 609.4816\n",
      "184\n",
      "Epoch: 46, Train Loss: 715.4797, Test Loss: 717.9504\n",
      "184\n",
      "Epoch: 47, Train Loss: 511.1828, Test Loss: 561.1129\n",
      "184\n",
      "Epoch: 48, Train Loss: 536.3567, Test Loss: 587.8964\n",
      "184\n",
      "Epoch: 49, Train Loss: 691.3539, Test Loss: 779.4055\n",
      "184\n",
      "Epoch: 50, Train Loss: 558.8997, Test Loss: 595.0870\n",
      "184\n",
      "Epoch: 51, Train Loss: 674.5670, Test Loss: 698.5092\n",
      "184\n",
      "Epoch: 52, Train Loss: 676.3573, Test Loss: 695.6211\n",
      "184\n",
      "Epoch: 53, Train Loss: 543.0001, Test Loss: 585.3108\n",
      "184\n",
      "Epoch: 54, Train Loss: 517.2604, Test Loss: 559.1118\n",
      "184\n",
      "Epoch: 55, Train Loss: 589.3063, Test Loss: 617.6801\n",
      "184\n",
      "Epoch: 56, Train Loss: 520.6764, Test Loss: 582.5848\n",
      "184\n",
      "Epoch: 57, Train Loss: 527.7616, Test Loss: 590.5106\n",
      "184\n",
      "Epoch: 58, Train Loss: 709.0027, Test Loss: 726.5799\n",
      "184\n",
      "Epoch: 59, Train Loss: 519.0505, Test Loss: 560.9990\n",
      "184\n",
      "Epoch: 60, Train Loss: 1127.2750, Test Loss: 1093.0013\n",
      "184\n",
      "Epoch: 61, Train Loss: 517.9016, Test Loss: 578.0389\n",
      "184\n",
      "Epoch: 62, Train Loss: 545.1088, Test Loss: 615.3174\n",
      "184\n",
      "Epoch: 63, Train Loss: 565.9751, Test Loss: 591.9055\n",
      "184\n",
      "Epoch: 64, Train Loss: 551.6009, Test Loss: 593.2192\n",
      "184\n",
      "Epoch: 65, Train Loss: 799.8107, Test Loss: 811.6909\n",
      "184\n",
      "Epoch: 66, Train Loss: 614.2863, Test Loss: 639.5152\n",
      "184\n",
      "Epoch: 67, Train Loss: 511.3424, Test Loss: 562.1568\n",
      "184\n",
      "Epoch: 68, Train Loss: 844.0078, Test Loss: 841.0588\n",
      "184\n",
      "Epoch: 69, Train Loss: 774.6561, Test Loss: 786.9316\n",
      "184\n",
      "Epoch: 70, Train Loss: 769.2853, Test Loss: 791.8978\n",
      "184\n",
      "Epoch: 71, Train Loss: 535.6315, Test Loss: 613.1687\n",
      "184\n",
      "Epoch: 72, Train Loss: 527.0380, Test Loss: 591.4271\n",
      "184\n",
      "Epoch: 73, Train Loss: 643.5961, Test Loss: 669.3647\n",
      "184\n",
      "Epoch: 74, Train Loss: 516.1992, Test Loss: 561.2027\n",
      "184\n",
      "Epoch: 75, Train Loss: 1151.8669, Test Loss: 1131.6284\n",
      "184\n",
      "Epoch: 76, Train Loss: 951.1657, Test Loss: 938.3495\n",
      "184\n",
      "Epoch: 77, Train Loss: 714.7436, Test Loss: 741.9041\n",
      "184\n",
      "Epoch: 78, Train Loss: 541.1297, Test Loss: 568.7011\n",
      "184\n",
      "Epoch: 79, Train Loss: 524.6574, Test Loss: 591.8297\n",
      "184\n",
      "Epoch: 80, Train Loss: 527.5291, Test Loss: 574.4953\n",
      "184\n",
      "Epoch: 81, Train Loss: 620.6833, Test Loss: 630.3696\n",
      "184\n",
      "Epoch: 82, Train Loss: 630.8914, Test Loss: 656.7169\n",
      "184\n",
      "Epoch: 83, Train Loss: 558.2749, Test Loss: 581.3906\n",
      "184\n",
      "Epoch: 84, Train Loss: 544.8676, Test Loss: 597.3851\n",
      "184\n",
      "Epoch: 85, Train Loss: 569.9315, Test Loss: 654.1983\n",
      "184\n",
      "Epoch: 86, Train Loss: 565.8771, Test Loss: 654.3291\n",
      "184\n",
      "Epoch: 87, Train Loss: 615.2234, Test Loss: 659.4751\n",
      "184\n",
      "Epoch: 88, Train Loss: 831.3474, Test Loss: 837.9486\n",
      "184\n",
      "Epoch: 89, Train Loss: 554.2117, Test Loss: 598.7974\n",
      "184\n",
      "Epoch: 90, Train Loss: 693.7379, Test Loss: 710.1920\n",
      "184\n",
      "Epoch: 91, Train Loss: 559.3792, Test Loss: 588.9792\n",
      "184\n",
      "Epoch: 92, Train Loss: 519.0837, Test Loss: 574.6958\n",
      "184\n",
      "Epoch: 93, Train Loss: 524.5228, Test Loss: 562.1676\n",
      "184\n",
      "Epoch: 94, Train Loss: 594.3019, Test Loss: 666.4844\n",
      "184\n",
      "Epoch: 95, Train Loss: 676.9441, Test Loss: 770.9212\n",
      "184\n",
      "Epoch: 96, Train Loss: 667.5803, Test Loss: 691.5732\n",
      "184\n",
      "Epoch: 97, Train Loss: 528.9264, Test Loss: 587.5178\n",
      "184\n",
      "Epoch: 98, Train Loss: 535.0558, Test Loss: 604.7176\n",
      "184\n",
      "Epoch: 99, Train Loss: 655.9035, Test Loss: 677.1555\n",
      "184\n",
      "Epoch: 100, Train Loss: 697.2990, Test Loss: 706.1480\n"
     ]
    }
   ],
   "source": [
    "# Przeprowadź trening i ewaluację\n",
    "for epoch in range(1, 101):\n",
    "    train(epoch)\n",
    "    train_loss = test(train_loader)\n",
    "    test_loss = test(test_loader)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "557cc9f2-0529-4ac2-ab08-d219ea883048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przewidywana wartość: 656.6989135742188\n",
      "Rzeczywista wartość: 874.5499877929688\n"
     ]
    }
   ],
   "source": [
    "sample_data = test_dataset[0]\n",
    "\n",
    "# Przeprowadzenie predykcji na przykładowym obiekcie\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = optimal_model(sample_data)[0][0].item()\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "print(\"Przewidywana wartość:\", prediction)\n",
    "print(\"Rzeczywista wartość:\", sample_data.y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f175333d-a83a-4a99-a172-800e4856b88c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
